{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "!python3 ../../../NLLP/experiment/PaddleNLP_UIE/finetune.py  \\\n",
    "    --device cpu \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --eval_steps 100 \\\n",
    "    --seed 42 \\\n",
    "    --model_name_or_path uie-base  \\\n",
    "    --output_dir ../../../NLLP/experiment/PaddleNLP_UIE/checkpoint/model_best \\\n",
    "    --train_path ../../../NLLP/experiment/PaddleNLP_UIE/training_data.txt \\\n",
    "    --dev_path ../../../NLLP/experiment/PaddleNLP_UIE/eval_data.txt  \\\n",
    "    --max_seq_length 512  \\\n",
    "    --per_device_eval_batch_size 16\\\n",
    "    --per_device_train_batch_size  16 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --label_names 'start_positions' 'end_positions' \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_export \\\n",
    "    --do_compress \\\n",
    "    --export_model_dir ../../../NLLP/experiment/PaddleNLP_UIE/checkpoint/model_best \\\n",
    "    --overwrite_output_dir \\\n",
    "    --disable_tqdm True \\\n",
    "    --metric_for_best_model eval_f1 \\\n",
    "    --load_best_model_at_end  True \\\n",
    "    --save_total_limit 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cfh00892302/Desktop/myWorkspace/env/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[33m[2023-05-03 15:48:47,111] [ WARNING]\u001b[0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,111] [    INFO]\u001b[0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,111] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,112] [    INFO]\u001b[0m -      Model Configuration Arguments      \u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,112] [    INFO]\u001b[0m - paddle commit id              :0e92adceae06b6b7463f2dc7790ffb0601730009\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,112] [    INFO]\u001b[0m - export_model_dir              :None\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,112] [    INFO]\u001b[0m - model_name_or_path            :uie-base\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,112] [    INFO]\u001b[0m - multilingual                  :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,112] [    INFO]\u001b[0m - \u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,112] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,112] [    INFO]\u001b[0m -       Data Configuration Arguments      \u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,112] [    INFO]\u001b[0m - paddle commit id              :0e92adceae06b6b7463f2dc7790ffb0601730009\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,112] [    INFO]\u001b[0m - dev_path                      :../../dev/data/cpu_data/eval_data.txt\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,112] [    INFO]\u001b[0m - max_seq_len                   :512\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,112] [    INFO]\u001b[0m - train_path                    :../../dev/data/cpu_data/training_data.txt\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,112] [    INFO]\u001b[0m - \u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,113] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,113] [    INFO]\u001b[0m -       Data Configuration Arguments      \u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,113] [    INFO]\u001b[0m - paddle commit id              :0e92adceae06b6b7463f2dc7790ffb0601730009\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,113] [    INFO]\u001b[0m - _no_sync_in_gradient_accumulation:True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,113] [    INFO]\u001b[0m - adam_beta1                    :0.9\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,113] [    INFO]\u001b[0m - adam_beta2                    :0.999\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,113] [    INFO]\u001b[0m - adam_epsilon                  :1e-08\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,114] [    INFO]\u001b[0m - bf16                          :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,114] [    INFO]\u001b[0m - bf16_full_eval                :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,114] [    INFO]\u001b[0m - current_device                :cpu\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,114] [    INFO]\u001b[0m - dataloader_drop_last          :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,114] [    INFO]\u001b[0m - dataloader_num_workers        :0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,114] [    INFO]\u001b[0m - device                        :cpu\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,114] [    INFO]\u001b[0m - disable_tqdm                  :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,114] [    INFO]\u001b[0m - do_eval                       :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,114] [    INFO]\u001b[0m - do_export                     :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,114] [    INFO]\u001b[0m - do_predict                    :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,114] [    INFO]\u001b[0m - do_train                      :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,115] [    INFO]\u001b[0m - eval_batch_size               :16\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,115] [    INFO]\u001b[0m - eval_steps                    :100\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,115] [    INFO]\u001b[0m - evaluation_strategy           :IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,115] [    INFO]\u001b[0m - flatten_param_grads           :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,115] [    INFO]\u001b[0m - fp16                          :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,115] [    INFO]\u001b[0m - fp16_full_eval                :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,115] [    INFO]\u001b[0m - fp16_opt_level                :O1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,115] [    INFO]\u001b[0m - gradient_accumulation_steps   :1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,115] [    INFO]\u001b[0m - greater_is_better             :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,115] [    INFO]\u001b[0m - ignore_data_skip              :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,115] [    INFO]\u001b[0m - label_names                   :['start_positions', 'end_positions']\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,115] [    INFO]\u001b[0m - lazy_data_processing          :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,116] [    INFO]\u001b[0m - learning_rate                 :1e-05\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,116] [    INFO]\u001b[0m - load_best_model_at_end        :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,116] [    INFO]\u001b[0m - local_process_index           :0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,116] [    INFO]\u001b[0m - local_rank                    :-1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,116] [    INFO]\u001b[0m - log_level                     :-1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,116] [    INFO]\u001b[0m - log_level_replica             :-1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,116] [    INFO]\u001b[0m - log_on_each_node              :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,116] [    INFO]\u001b[0m - logging_dir                   :/Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/runs/May03_15-48-47_CFH00892302de-MacBook-Pro.local\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,116] [    INFO]\u001b[0m - logging_first_step            :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,117] [    INFO]\u001b[0m - logging_steps                 :10\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,117] [    INFO]\u001b[0m - logging_strategy              :IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,117] [    INFO]\u001b[0m - lr_scheduler_type             :SchedulerType.LINEAR\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,117] [    INFO]\u001b[0m - max_grad_norm                 :1.0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,117] [    INFO]\u001b[0m - max_steps                     :-1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,117] [    INFO]\u001b[0m - metric_for_best_model         :eval_f1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,117] [    INFO]\u001b[0m - minimum_eval_times            :None\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,117] [    INFO]\u001b[0m - no_cuda                       :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,117] [    INFO]\u001b[0m - num_train_epochs              :1.0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,118] [    INFO]\u001b[0m - optim                         :OptimizerNames.ADAMW\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,118] [    INFO]\u001b[0m - output_dir                    :/Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,118] [    INFO]\u001b[0m - overwrite_output_dir          :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,118] [    INFO]\u001b[0m - past_index                    :-1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,118] [    INFO]\u001b[0m - per_device_eval_batch_size    :16\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,118] [    INFO]\u001b[0m - per_device_train_batch_size   :16\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,118] [    INFO]\u001b[0m - prediction_loss_only          :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,118] [    INFO]\u001b[0m - process_index                 :0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,118] [    INFO]\u001b[0m - recompute                     :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,118] [    INFO]\u001b[0m - remove_unused_columns         :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,118] [    INFO]\u001b[0m - report_to                     :['visualdl']\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,119] [    INFO]\u001b[0m - resume_from_checkpoint        :None\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,119] [    INFO]\u001b[0m - run_name                      :/Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,119] [    INFO]\u001b[0m - save_on_each_node             :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,119] [    INFO]\u001b[0m - save_steps                    :100\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,119] [    INFO]\u001b[0m - save_strategy                 :IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,119] [    INFO]\u001b[0m - save_total_limit              :1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,119] [    INFO]\u001b[0m - scale_loss                    :32768\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,119] [    INFO]\u001b[0m - seed                          :42\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,119] [    INFO]\u001b[0m - sharding                      :[]\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,119] [    INFO]\u001b[0m - sharding_degree               :-1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,120] [    INFO]\u001b[0m - should_log                    :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,120] [    INFO]\u001b[0m - should_save                   :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,120] [    INFO]\u001b[0m - skip_memory_metrics           :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,120] [    INFO]\u001b[0m - train_batch_size              :16\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,120] [    INFO]\u001b[0m - warmup_ratio                  :0.0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,120] [    INFO]\u001b[0m - warmup_steps                  :0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,120] [    INFO]\u001b[0m - weight_decay                  :0.0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,120] [    INFO]\u001b[0m - world_size                    :1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,120] [    INFO]\u001b[0m - \u001b[0m\n",
      "2023-05-03 15:48:47 |\u001b[32m INFO     \u001b[0m| __main__ | Process rank: -1, device: cpu, world_size: 1, distributed training: False, 16-bits training: False\n",
      "[2023-05-03 15:48:47,120] [    INFO] finetune.py:73 - Process rank: -1, device: cpu, world_size: 1, distributed training: False, 16-bits training: False\n",
      "\u001b[32m[2023-05-03 15:48:47,122] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'uie-base'.\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,122] [    INFO]\u001b[0m - Already cached /Users/cfh00892302/.paddlenlp/models/uie-base/uie-base/ernie_3.0_base_zh_vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,150] [    INFO]\u001b[0m - tokenizer config file saved in /Users/cfh00892302/.paddlenlp/models/uie-base/uie-base/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,152] [    INFO]\u001b[0m - Special tokens file saved in /Users/cfh00892302/.paddlenlp/models/uie-base/uie-base/special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,154] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'uie-base'.\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:47,154] [    INFO]\u001b[0m - Model config ErnieConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"enable_recompute\": false,\n",
      "  \"fuse\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"ernie\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"pool_act\": \"tanh\",\n",
      "  \"task_id\": 0,\n",
      "  \"task_type_vocab_size\": 3,\n",
      "  \"type_vocab_size\": 4,\n",
      "  \"use_task_id\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[33m[2023-05-03 15:48:56,815] [ WARNING]\u001b[0m - Some weights of the model checkpoint at uie-base were not used when initializing ErnieModel: ['linear_start.weight', 'linear_start.bias', 'linear_end.bias', 'linear_end.weight']\n",
      "- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:56,815] [    INFO]\u001b[0m - All the weights of ErnieModel were initialized from the model checkpoint at uie-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieModel for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,096] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,096] [    INFO]\u001b[0m -     Training Configuration Arguments    \u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,096] [    INFO]\u001b[0m - paddle commit id              :0e92adceae06b6b7463f2dc7790ffb0601730009\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,096] [    INFO]\u001b[0m - _no_sync_in_gradient_accumulation:True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,096] [    INFO]\u001b[0m - adam_beta1                    :0.9\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,096] [    INFO]\u001b[0m - adam_beta2                    :0.999\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,096] [    INFO]\u001b[0m - adam_epsilon                  :1e-08\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,096] [    INFO]\u001b[0m - bf16                          :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,096] [    INFO]\u001b[0m - bf16_full_eval                :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,096] [    INFO]\u001b[0m - current_device                :cpu\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,097] [    INFO]\u001b[0m - dataloader_drop_last          :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,097] [    INFO]\u001b[0m - dataloader_num_workers        :0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,097] [    INFO]\u001b[0m - device                        :cpu\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,097] [    INFO]\u001b[0m - disable_tqdm                  :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,097] [    INFO]\u001b[0m - do_eval                       :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,097] [    INFO]\u001b[0m - do_export                     :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,097] [    INFO]\u001b[0m - do_predict                    :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,097] [    INFO]\u001b[0m - do_train                      :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,097] [    INFO]\u001b[0m - eval_batch_size               :16\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,097] [    INFO]\u001b[0m - eval_steps                    :100\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,098] [    INFO]\u001b[0m - evaluation_strategy           :IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,098] [    INFO]\u001b[0m - flatten_param_grads           :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,098] [    INFO]\u001b[0m - fp16                          :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,098] [    INFO]\u001b[0m - fp16_full_eval                :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,098] [    INFO]\u001b[0m - fp16_opt_level                :O1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,098] [    INFO]\u001b[0m - gradient_accumulation_steps   :1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,098] [    INFO]\u001b[0m - greater_is_better             :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,098] [    INFO]\u001b[0m - ignore_data_skip              :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,098] [    INFO]\u001b[0m - label_names                   :['start_positions', 'end_positions']\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,098] [    INFO]\u001b[0m - lazy_data_processing          :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,098] [    INFO]\u001b[0m - learning_rate                 :1e-05\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,098] [    INFO]\u001b[0m - load_best_model_at_end        :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,098] [    INFO]\u001b[0m - local_process_index           :0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,099] [    INFO]\u001b[0m - local_rank                    :-1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,099] [    INFO]\u001b[0m - log_level                     :-1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,099] [    INFO]\u001b[0m - log_level_replica             :-1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,099] [    INFO]\u001b[0m - log_on_each_node              :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,099] [    INFO]\u001b[0m - logging_dir                   :/Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/runs/May03_15-48-47_CFH00892302de-MacBook-Pro.local\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,099] [    INFO]\u001b[0m - logging_first_step            :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,099] [    INFO]\u001b[0m - logging_steps                 :10\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,099] [    INFO]\u001b[0m - logging_strategy              :IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,099] [    INFO]\u001b[0m - lr_scheduler_type             :SchedulerType.LINEAR\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,099] [    INFO]\u001b[0m - max_grad_norm                 :1.0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,099] [    INFO]\u001b[0m - max_steps                     :-1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,099] [    INFO]\u001b[0m - metric_for_best_model         :eval_f1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,100] [    INFO]\u001b[0m - minimum_eval_times            :None\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,100] [    INFO]\u001b[0m - no_cuda                       :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,100] [    INFO]\u001b[0m - num_train_epochs              :1.0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,100] [    INFO]\u001b[0m - optim                         :OptimizerNames.ADAMW\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,100] [    INFO]\u001b[0m - output_dir                    :/Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint230503154847/\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,100] [    INFO]\u001b[0m - overwrite_output_dir          :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,100] [    INFO]\u001b[0m - past_index                    :-1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,100] [    INFO]\u001b[0m - per_device_eval_batch_size    :16\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,100] [    INFO]\u001b[0m - per_device_train_batch_size   :16\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,100] [    INFO]\u001b[0m - prediction_loss_only          :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,100] [    INFO]\u001b[0m - process_index                 :0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,100] [    INFO]\u001b[0m - recompute                     :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,100] [    INFO]\u001b[0m - remove_unused_columns         :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,101] [    INFO]\u001b[0m - report_to                     :['visualdl']\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,101] [    INFO]\u001b[0m - resume_from_checkpoint        :None\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,101] [    INFO]\u001b[0m - run_name                      :/Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,101] [    INFO]\u001b[0m - save_on_each_node             :False\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,101] [    INFO]\u001b[0m - save_steps                    :100\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,101] [    INFO]\u001b[0m - save_strategy                 :IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,101] [    INFO]\u001b[0m - save_total_limit              :1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,101] [    INFO]\u001b[0m - scale_loss                    :32768\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,101] [    INFO]\u001b[0m - seed                          :42\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,101] [    INFO]\u001b[0m - sharding                      :[]\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,101] [    INFO]\u001b[0m - sharding_degree               :-1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,101] [    INFO]\u001b[0m - should_log                    :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,101] [    INFO]\u001b[0m - should_save                   :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,101] [    INFO]\u001b[0m - skip_memory_metrics           :True\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,102] [    INFO]\u001b[0m - train_batch_size              :16\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,102] [    INFO]\u001b[0m - warmup_ratio                  :0.0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,102] [    INFO]\u001b[0m - warmup_steps                  :0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,102] [    INFO]\u001b[0m - weight_decay                  :0.0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,102] [    INFO]\u001b[0m - world_size                    :1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,102] [    INFO]\u001b[0m - \u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,103] [    INFO]\u001b[0m - ***** Running training *****\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,104] [    INFO]\u001b[0m -   Num examples = 1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,104] [    INFO]\u001b[0m -   Num Epochs = 1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,104] [    INFO]\u001b[0m -   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,104] [    INFO]\u001b[0m -   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,104] [    INFO]\u001b[0m -   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,104] [    INFO]\u001b[0m -   Total optimization steps = 1.0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,104] [    INFO]\u001b[0m -   Total num train samples = 1.0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:48:57,106] [    INFO]\u001b[0m -   Number of trainable parameters = 117944832\u001b[0m\n",
      "2023-05-03 15:48:59 |\u001b[36m DEBUG    \u001b[0m| __main__ | In uie_loss_func, log type(outputs): <class 'tuple'>, type(labels): <class 'tuple'>\n",
      "[2023-05-03 15:48:59,808] [   DEBUG] finetune.py:102 - In uie_loss_func, log type(outputs): <class 'tuple'>, type(labels): <class 'tuple'>\n",
      "\u001b[32m[2023-05-03 15:49:31,001] [    INFO]\u001b[0m - \n",
      "Training completed. \n",
      "\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:31,008] [    INFO]\u001b[0m - train_runtime: 33.8956, train_samples_per_second: 0.03, train_steps_per_second: 0.03, train_loss: 0.016264397650957108, epoch: 1.0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:31,008] [    INFO]\u001b[0m - Saving model checkpoint to /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint230503154847/\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:31,012] [    INFO]\u001b[0m - Configuration saved in /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint230503154847/config.json\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:34,054] [    INFO]\u001b[0m - tokenizer config file saved in /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint230503154847/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:34,055] [    INFO]\u001b[0m - Special tokens file saved in /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint230503154847/special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:34,059] [    INFO]\u001b[0m - ***** train metrics *****\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:34,059] [    INFO]\u001b[0m -   epoch                    =        1.0\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:34,059] [    INFO]\u001b[0m -   train_loss               =     0.0163\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:34,060] [    INFO]\u001b[0m -   train_runtime            = 0:00:33.89\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:34,060] [    INFO]\u001b[0m -   train_samples_per_second =       0.03\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:34,060] [    INFO]\u001b[0m -   train_steps_per_second   =       0.03\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:34,061] [    INFO]\u001b[0m - ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:34,062] [    INFO]\u001b[0m -   Num examples = 1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:34,062] [    INFO]\u001b[0m -   Total prediction steps = 1\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:34,062] [    INFO]\u001b[0m -   Pre device batch size = 16\u001b[0m\n",
      "\u001b[32m[2023-05-03 15:49:34,062] [    INFO]\u001b[0m -   Total Batch size = 16\u001b[0m\n",
      "2023-05-03 15:49:36 |\u001b[36m DEBUG    \u001b[0m| __main__ | In uie_loss_func, log type(outputs): <class 'tuple'>, type(labels): <class 'tuple'>\n",
      "[2023-05-03 15:49:36,328] [   DEBUG] finetune.py:102 - In uie_loss_func, log type(outputs): <class 'tuple'>, type(labels): <class 'tuple'>\n",
      "2023-05-03 15:49:36 |\u001b[36m DEBUG    \u001b[0m| training_callbacks | Test1\n",
      "[2023-05-03 15:49:36,334] [   DEBUG] training_callbacks.py:21 - Test1\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/model/finetune.py\", line 224, in <module>\n",
      "    finetune(\n",
      "  File \"/Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/model/finetune.py\", line 176, in finetune\n",
      "    eval_metrics = trainer.evaluate()\n",
      "  File \"/Users/cfh00892302/Desktop/myWorkspace/env/lib/python3.9/site-packages/paddlenlp/trainer/trainer.py\", line 1588, in evaluate\n",
      "    output = self.evaluation_loop(\n",
      "  File \"/Users/cfh00892302/Desktop/myWorkspace/env/lib/python3.9/site-packages/paddlenlp/trainer/trainer.py\", line 1768, in evaluation_loop\n",
      "    metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))\n",
      "  File \"/Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/model/training_callbacks.py\", line 23, in SpanEvaluator_metrics\n",
      "    num_correct, num_infer, num_label = metric.compute(start_prob, end_prob, start_ids, end_ids)\n",
      "  File \"/Users/cfh00892302/Desktop/myWorkspace/env/lib/python3.9/site-packages/paddlenlp/metrics/span.py\", line 46, in compute\n",
      "    [_correct, _infer, _label] = self.eval_span(\n",
      "  File \"/Users/cfh00892302/Desktop/myWorkspace/env/lib/python3.9/site-packages/paddlenlp/metrics/span.py\", line 70, in eval_span\n",
      "    pred_set = get_span(predict_start_ids, predict_end_ids)\n",
      "  File \"/Users/cfh00892302/Desktop/myWorkspace/env/lib/python3.9/site-packages/paddlenlp/utils/tools.py\", line 238, in get_span\n",
      "    if start_id < end_id:\n",
      "TypeError: '<' not supported between instances of 'list' and 'int'\n"
     ]
    }
   ],
   "source": [
    "# toy test\n",
    "!python3 finetune.py  \\\n",
    "    --device cpu \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --eval_steps 100 \\\n",
    "    --seed 42 \\\n",
    "    --model_name_or_path uie-base  \\\n",
    "    --train_path ../../dev/data/cpu_data/training_data.txt \\\n",
    "    --dev_path ../../dev/data/cpu_data/eval_data.txt  \\\n",
    "    --max_seq_len 512  \\\n",
    "    --per_device_eval_batch_size 16\\\n",
    "    --per_device_train_batch_size  16 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --label_names 'start_positions' 'end_positions' \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_export \\\n",
    "    --overwrite_output_dir \\\n",
    "    --disable_tqdm True \\\n",
    "    --metric_for_best_model eval_f1 \\\n",
    "    --load_best_model_at_end  True \\\n",
    "    --save_total_limit 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d58828fd0c5f7af717daf8982e0a9ccf3c174b5c7bbe63b6216d1f875908829"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
