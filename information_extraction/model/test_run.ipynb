{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "!python3 ../../../NLLP/experiment/PaddleNLP_UIE/finetune.py  \\\n",
    "    --device cpu \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --eval_steps 100 \\\n",
    "    --seed 42 \\\n",
    "    --model_name_or_path uie-base  \\\n",
    "    --output_dir ../../../NLLP/experiment/PaddleNLP_UIE/checkpoint/model_best \\\n",
    "    --train_path ../../../NLLP/experiment/PaddleNLP_UIE/training_data.txt \\\n",
    "    --dev_path ../../../NLLP/experiment/PaddleNLP_UIE/eval_data.txt  \\\n",
    "    --max_seq_length 512  \\\n",
    "    --per_device_eval_batch_size 16\\\n",
    "    --per_device_train_batch_size  16 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --label_names 'start_positions' 'end_positions' \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_export \\\n",
    "    --do_compress \\\n",
    "    --export_model_dir ../../../NLLP/experiment/PaddleNLP_UIE/checkpoint/model_best \\\n",
    "    --overwrite_output_dir \\\n",
    "    --disable_tqdm True \\\n",
    "    --metric_for_best_model eval_f1 \\\n",
    "    --load_best_model_at_end  True \\\n",
    "    --save_total_limit 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2023-05-04 03:31:12,673] [ WARNING]\u001b[0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,673] [    INFO]\u001b[0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,673] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,673] [    INFO]\u001b[0m -      Model Configuration Arguments      \u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,673] [    INFO]\u001b[0m - paddle commit id              :4596b9a22540fb0ea5d369c3c804544de61d03d0\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,673] [    INFO]\u001b[0m - export_model_dir              :None\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,673] [    INFO]\u001b[0m - model_name_or_path            :uie-base\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,673] [    INFO]\u001b[0m - multilingual                  :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,673] [    INFO]\u001b[0m - \u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,673] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,674] [    INFO]\u001b[0m -       Data Configuration Arguments      \u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,674] [    INFO]\u001b[0m - paddle commit id              :4596b9a22540fb0ea5d369c3c804544de61d03d0\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,674] [    INFO]\u001b[0m - dev_path                      :../data/data_for_one_shot_example/eval_data.txt\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,674] [    INFO]\u001b[0m - max_seq_len                   :512\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,674] [    INFO]\u001b[0m - train_path                    :../data/data_for_one_shot_example/training_data.txt\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,674] [    INFO]\u001b[0m - \u001b[0m\n",
      "2023-05-04 03:31:12 |\u001b[32m INFO     \u001b[0m| __main__ | Process rank: -1, device: gpu, world_size: 1, distributed training: False, 16-bits training: False\n",
      "\u001b[32m[2023-05-04 03:31:12,675] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'uie-base'.\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,675] [    INFO]\u001b[0m - Already cached /home/ubuntu/.paddlenlp/models/uie-base/uie-base/ernie_3.0_base_zh_vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,698] [    INFO]\u001b[0m - tokenizer config file saved in /home/ubuntu/.paddlenlp/models/uie-base/uie-base/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,698] [    INFO]\u001b[0m - Special tokens file saved in /home/ubuntu/.paddlenlp/models/uie-base/uie-base/special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,699] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'uie-base'.\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:12,700] [    INFO]\u001b[0m - Model config ErnieConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"enable_recompute\": false,\n",
      "  \"fuse\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"ernie\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"pool_act\": \"tanh\",\n",
      "  \"task_id\": 0,\n",
      "  \"task_type_vocab_size\": 3,\n",
      "  \"type_vocab_size\": 4,\n",
      "  \"use_task_id\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\u001b[0m\n",
      "W0504 03:31:13.959422  3912 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.7, Runtime API Version: 10.2\n",
      "W0504 03:31:13.964332  3912 gpu_resources.cc:91] device: 0, cuDNN Version: 8.6.\n",
      "\u001b[33m[2023-05-04 03:31:14,588] [ WARNING]\u001b[0m - Some weights of the model checkpoint at uie-base were not used when initializing ErnieModel: ['linear_end.weight', 'linear_end.bias', 'linear_start.bias', 'linear_start.weight']\n",
      "- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,588] [    INFO]\u001b[0m - All the weights of ErnieModel were initialized from the model checkpoint at uie-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieModel for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,667] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,667] [    INFO]\u001b[0m -     Training Configuration Arguments    \u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,668] [    INFO]\u001b[0m - paddle commit id              :4596b9a22540fb0ea5d369c3c804544de61d03d0\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,668] [    INFO]\u001b[0m - _no_sync_in_gradient_accumulation:True\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,668] [    INFO]\u001b[0m - adam_beta1                    :0.9\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,668] [    INFO]\u001b[0m - adam_beta2                    :0.999\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,668] [    INFO]\u001b[0m - adam_epsilon                  :1e-08\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,668] [    INFO]\u001b[0m - bf16                          :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,668] [    INFO]\u001b[0m - bf16_full_eval                :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,668] [    INFO]\u001b[0m - current_device                :gpu:0\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,668] [    INFO]\u001b[0m - dataloader_drop_last          :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,668] [    INFO]\u001b[0m - dataloader_num_workers        :0\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,668] [    INFO]\u001b[0m - device                        :gpu\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,668] [    INFO]\u001b[0m - disable_tqdm                  :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,668] [    INFO]\u001b[0m - do_eval                       :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,668] [    INFO]\u001b[0m - do_export                     :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,669] [    INFO]\u001b[0m - do_predict                    :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,669] [    INFO]\u001b[0m - do_train                      :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,669] [    INFO]\u001b[0m - eval_batch_size               :16\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,669] [    INFO]\u001b[0m - eval_steps                    :100\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,669] [    INFO]\u001b[0m - evaluation_strategy           :IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,669] [    INFO]\u001b[0m - flatten_param_grads           :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,669] [    INFO]\u001b[0m - fp16                          :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,669] [    INFO]\u001b[0m - fp16_full_eval                :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,669] [    INFO]\u001b[0m - fp16_opt_level                :O1\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,669] [    INFO]\u001b[0m - gradient_accumulation_steps   :1\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,669] [    INFO]\u001b[0m - greater_is_better             :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,669] [    INFO]\u001b[0m - ignore_data_skip              :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,669] [    INFO]\u001b[0m - label_names                   :['start_positions', 'end_positions']\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,669] [    INFO]\u001b[0m - lazy_data_processing          :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,669] [    INFO]\u001b[0m - learning_rate                 :1e-05\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,670] [    INFO]\u001b[0m - load_best_model_at_end        :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,670] [    INFO]\u001b[0m - local_process_index           :0\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,670] [    INFO]\u001b[0m - local_rank                    :-1\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,670] [    INFO]\u001b[0m - log_level                     :-1\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,670] [    INFO]\u001b[0m - log_level_replica             :-1\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,670] [    INFO]\u001b[0m - log_on_each_node              :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,670] [    INFO]\u001b[0m - logging_dir                   :/home/ubuntu/work/Chinese-Verdict-NLP/information_extraction/results/checkpoint/runs/May04_03-31-12_ip-172-31-32-119\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,670] [    INFO]\u001b[0m - logging_first_step            :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,670] [    INFO]\u001b[0m - logging_steps                 :10\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,670] [    INFO]\u001b[0m - logging_strategy              :IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,670] [    INFO]\u001b[0m - lr_scheduler_type             :SchedulerType.LINEAR\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,670] [    INFO]\u001b[0m - max_grad_norm                 :1.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,670] [    INFO]\u001b[0m - max_steps                     :-1\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,670] [    INFO]\u001b[0m - metric_for_best_model         :eval_f1\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,670] [    INFO]\u001b[0m - minimum_eval_times            :None\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - no_cuda                       :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - num_train_epochs              :2.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - optim                         :OptimizerNames.ADAMW\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - output_dir                    :/home/ubuntu/work/Chinese-Verdict-NLP/information_extraction/results/checkpoint/\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - overwrite_output_dir          :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - past_index                    :-1\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - per_device_eval_batch_size    :16\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - per_device_train_batch_size   :16\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - prediction_loss_only          :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - process_index                 :0\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - recompute                     :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - remove_unused_columns         :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - report_to                     :['visualdl']\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - resume_from_checkpoint        :None\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - run_name                      :/home/ubuntu/work/Chinese-Verdict-NLP/information_extraction/results/checkpoint/\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,671] [    INFO]\u001b[0m - save_on_each_node             :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - save_steps                    :100\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - save_strategy                 :IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - save_total_limit              :1\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - scale_loss                    :32768\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - seed                          :42\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - sharding                      :[]\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - sharding_degree               :-1\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - should_log                    :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - should_save                   :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - skip_memory_metrics           :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - train_batch_size              :16\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - warmup_ratio                  :0.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - warmup_steps                  :0\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - weight_decay                  :0.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - world_size                    :1\u001b[0m\n",
      "\u001b[32m[2023-05-04 03:31:14,672] [    INFO]\u001b[0m - \u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"finetune.py\", line 219, in <module>\n",
      "    finetune(\n",
      "  File \"finetune.py\", line 146, in finetune\n",
      "    raise ValueError(\n",
      "ValueError: Output directory (/home/ubuntu/work/Chinese-Verdict-NLP/information_extraction/results/checkpoint/) already exists and is not empty. Use --overwrite_output_dir to overcome.\n"
     ]
    }
   ],
   "source": [
    "# toy test\n",
    "!python3 finetune.py  \\\n",
    "    --device gpu \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --eval_steps 100 \\\n",
    "    --seed 42 \\\n",
    "    --model_name_or_path uie-base  \\\n",
    "    --train_path ../data/data_for_one_shot_example/training_data.txt \\\n",
    "    --dev_path ../data/data_for_one_shot_example/eval_data.txt  \\\n",
    "    --max_seq_len 512  \\\n",
    "    --per_device_eval_batch_size 16\\\n",
    "    --per_device_train_batch_size  16 \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --label_names 'start_positions' 'end_positions' \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_export \\\n",
    "    --disable_tqdm True \\\n",
    "    --metric_for_best_model eval_f1 \\\n",
    "    --load_best_model_at_end  True \\\n",
    "    --save_total_limit 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv_pytorch': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf46acb013efd0588f6afec6d5cfc3a8d59196ff594941befc7df90c31a68b6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
