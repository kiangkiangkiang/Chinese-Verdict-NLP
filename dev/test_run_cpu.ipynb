{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cfh00892302/Desktop/myWorkspace/env/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[33m[2023-05-11 12:47:20,941] [ WARNING]\u001b[0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:20,941] [    INFO]\u001b[0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:20,941] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:20,941] [    INFO]\u001b[0m -      Model Configuration Arguments      \u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:20,941] [    INFO]\u001b[0m - paddle commit id              :0e92adceae06b6b7463f2dc7790ffb0601730009\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:20,941] [    INFO]\u001b[0m - export_model_dir              :None\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:20,942] [    INFO]\u001b[0m - model_name_or_path            :uie-base\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:20,942] [    INFO]\u001b[0m - multilingual                  :False\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:20,942] [    INFO]\u001b[0m - \u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:20,942] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:20,942] [    INFO]\u001b[0m -       Data Configuration Arguments      \u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:20,942] [    INFO]\u001b[0m - paddle commit id              :0e92adceae06b6b7463f2dc7790ffb0601730009\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:20,942] [    INFO]\u001b[0m - dev_path                      :./data/cpu_data/eval_data.txt\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:20,942] [    INFO]\u001b[0m - max_seq_len                   :512\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:20,943] [    INFO]\u001b[0m - train_path                    :./data/cpu_data/training_data.txt\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:20,943] [    INFO]\u001b[0m - \u001b[0m\n",
      "\u001b[35m[2023-05-11 12:47:22,500] [   DEBUG]\u001b[0m - exp_id=2\u001b[0m\n",
      "\u001b[35m[2023-05-11 12:47:22,501] [   DEBUG]\u001b[0m - Success to set up mlflow.\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:22,501] [    INFO]\u001b[0m - Process rank: -1, device: cpu, world_size: 1, distributed training: False, 16-bits training: False\u001b[0m\n",
      "[{'text': '，以支付', 'start': 9, 'end': 13}]\n",
      "[{'text': '臺北地', 'start': 2, 'end': 5}]\n",
      "\u001b[32m[2023-05-11 12:47:22,503] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'uie-base'.\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:22,503] [    INFO]\u001b[0m - Already cached /Users/cfh00892302/.paddlenlp/models/uie-base/uie-base/ernie_3.0_base_zh_vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:22,535] [    INFO]\u001b[0m - tokenizer config file saved in /Users/cfh00892302/.paddlenlp/models/uie-base/uie-base/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:22,535] [    INFO]\u001b[0m - Special tokens file saved in /Users/cfh00892302/.paddlenlp/models/uie-base/uie-base/special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:22,537] [    INFO]\u001b[0m - Model config ErnieConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"enable_recompute\": false,\n",
      "  \"fuse\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"ernie\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"pool_act\": \"tanh\",\n",
      "  \"task_id\": 0,\n",
      "  \"task_type_vocab_size\": 3,\n",
      "  \"type_vocab_size\": 4,\n",
      "  \"use_task_id\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,563] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing UIE.\n",
      "\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,563] [    INFO]\u001b[0m - All the weights of UIE were initialized from the model checkpoint at uie-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UIE for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,736] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,737] [    INFO]\u001b[0m -     Training Configuration Arguments    \u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,737] [    INFO]\u001b[0m - paddle commit id              :0e92adceae06b6b7463f2dc7790ffb0601730009\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,737] [    INFO]\u001b[0m - _no_sync_in_gradient_accumulation:True\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,737] [    INFO]\u001b[0m - adam_beta1                    :0.9\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,737] [    INFO]\u001b[0m - adam_beta2                    :0.999\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,737] [    INFO]\u001b[0m - adam_epsilon                  :1e-08\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,737] [    INFO]\u001b[0m - bf16                          :False\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,737] [    INFO]\u001b[0m - bf16_full_eval                :False\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,737] [    INFO]\u001b[0m - current_device                :cpu\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,738] [    INFO]\u001b[0m - dataloader_drop_last          :False\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,738] [    INFO]\u001b[0m - dataloader_num_workers        :0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,738] [    INFO]\u001b[0m - device                        :cpu\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,738] [    INFO]\u001b[0m - disable_tqdm                  :True\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,738] [    INFO]\u001b[0m - do_eval                       :True\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,738] [    INFO]\u001b[0m - do_export                     :True\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,738] [    INFO]\u001b[0m - do_predict                    :False\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,738] [    INFO]\u001b[0m - do_train                      :True\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,738] [    INFO]\u001b[0m - eval_batch_size               :16\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,738] [    INFO]\u001b[0m - eval_steps                    :100\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,738] [    INFO]\u001b[0m - evaluation_strategy           :IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,739] [    INFO]\u001b[0m - flatten_param_grads           :False\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,739] [    INFO]\u001b[0m - fp16                          :False\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,739] [    INFO]\u001b[0m - fp16_full_eval                :False\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,739] [    INFO]\u001b[0m - fp16_opt_level                :O1\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,739] [    INFO]\u001b[0m - gradient_accumulation_steps   :1\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,739] [    INFO]\u001b[0m - greater_is_better             :True\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,739] [    INFO]\u001b[0m - ignore_data_skip              :False\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,739] [    INFO]\u001b[0m - label_names                   :['start_positions', 'end_positions']\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,739] [    INFO]\u001b[0m - lazy_data_processing          :True\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,739] [    INFO]\u001b[0m - learning_rate                 :1e-05\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,739] [    INFO]\u001b[0m - load_best_model_at_end        :True\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,739] [    INFO]\u001b[0m - local_process_index           :0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,739] [    INFO]\u001b[0m - local_rank                    :-1\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,740] [    INFO]\u001b[0m - log_level                     :-1\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,740] [    INFO]\u001b[0m - log_level_replica             :-1\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,740] [    INFO]\u001b[0m - log_on_each_node              :True\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,740] [    INFO]\u001b[0m - logging_dir                   :/Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/runs/May11_12-47-20_CFH00892302de-MacBook-Pro.local\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,740] [    INFO]\u001b[0m - logging_first_step            :False\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,740] [    INFO]\u001b[0m - logging_steps                 :10\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,740] [    INFO]\u001b[0m - logging_strategy              :IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,740] [    INFO]\u001b[0m - lr_scheduler_type             :SchedulerType.LINEAR\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,740] [    INFO]\u001b[0m - max_grad_norm                 :1.0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,740] [    INFO]\u001b[0m - max_steps                     :-1\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,740] [    INFO]\u001b[0m - metric_for_best_model         :eval_f1\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,740] [    INFO]\u001b[0m - minimum_eval_times            :None\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,740] [    INFO]\u001b[0m - no_cuda                       :False\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,741] [    INFO]\u001b[0m - num_train_epochs              :2.0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,741] [    INFO]\u001b[0m - optim                         :OptimizerNames.ADAMW\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,741] [    INFO]\u001b[0m - output_dir                    :/Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,741] [    INFO]\u001b[0m - overwrite_output_dir          :True\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,741] [    INFO]\u001b[0m - past_index                    :-1\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,741] [    INFO]\u001b[0m - per_device_eval_batch_size    :16\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,741] [    INFO]\u001b[0m - per_device_train_batch_size   :16\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,741] [    INFO]\u001b[0m - prediction_loss_only          :False\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,741] [    INFO]\u001b[0m - process_index                 :0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,741] [    INFO]\u001b[0m - recompute                     :False\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,741] [    INFO]\u001b[0m - remove_unused_columns         :True\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,741] [    INFO]\u001b[0m - report_to                     :['visualdl']\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,741] [    INFO]\u001b[0m - resume_from_checkpoint        :None\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,742] [    INFO]\u001b[0m - run_name                      :/Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,742] [    INFO]\u001b[0m - save_on_each_node             :False\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,742] [    INFO]\u001b[0m - save_steps                    :100\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,742] [    INFO]\u001b[0m - save_strategy                 :IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,742] [    INFO]\u001b[0m - save_total_limit              :1\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,742] [    INFO]\u001b[0m - scale_loss                    :32768\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,742] [    INFO]\u001b[0m - seed                          :42\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,742] [    INFO]\u001b[0m - sharding                      :[]\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,742] [    INFO]\u001b[0m - sharding_degree               :-1\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,742] [    INFO]\u001b[0m - should_log                    :True\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,742] [    INFO]\u001b[0m - should_save                   :True\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,742] [    INFO]\u001b[0m - skip_memory_metrics           :True\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,742] [    INFO]\u001b[0m - train_batch_size              :16\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,743] [    INFO]\u001b[0m - warmup_ratio                  :0.0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,743] [    INFO]\u001b[0m - warmup_steps                  :0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,743] [    INFO]\u001b[0m - weight_decay                  :0.0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,743] [    INFO]\u001b[0m - world_size                    :1\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:35,743] [    INFO]\u001b[0m - \u001b[0m\n",
      "\u001b[35m[2023-05-11 12:47:35,743] [   DEBUG]\u001b[0m - chechpoint: None\u001b[0m\n",
      "\u001b[35m[2023-05-11 12:47:35,743] [   DEBUG]\u001b[0m - last_checkpoint: None\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:39,337] [    INFO]\u001b[0m - ***** Running training *****\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:39,338] [    INFO]\u001b[0m -   Num examples = 1\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:39,338] [    INFO]\u001b[0m -   Num Epochs = 2\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:39,338] [    INFO]\u001b[0m -   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:39,338] [    INFO]\u001b[0m -   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:39,339] [    INFO]\u001b[0m -   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:39,339] [    INFO]\u001b[0m -   Total optimization steps = 2.0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:39,339] [    INFO]\u001b[0m -   Total num train samples = 2.0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:47:39,342] [    INFO]\u001b[0m -   Number of trainable parameters = 117946370\u001b[0m\n",
      "\u001b[35m[2023-05-11 12:47:42,117] [   DEBUG]\u001b[0m - in mlflow loss\u001b[0m\n",
      "\u001b[35m[2023-05-11 12:48:15,121] [   DEBUG]\u001b[0m - in mlflow loss\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:45,780] [    INFO]\u001b[0m - \n",
      "Training completed. \n",
      "\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:45,783] [    INFO]\u001b[0m - train_runtime: 66.4381, train_samples_per_second: 0.03, train_steps_per_second: 0.03, train_loss: 0.016271773725748062, epoch: 2.0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:46,003] [    INFO]\u001b[0m - Saving model checkpoint to /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:46,005] [    INFO]\u001b[0m - Configuration saved in /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/config.json\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:49,143] [    INFO]\u001b[0m - tokenizer config file saved in /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:49,146] [    INFO]\u001b[0m - Special tokens file saved in /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:49,151] [    INFO]\u001b[0m - ***** train metrics *****\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:49,151] [    INFO]\u001b[0m -   epoch                    =        2.0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:49,151] [    INFO]\u001b[0m -   train_loss               =     0.0163\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:49,151] [    INFO]\u001b[0m -   train_runtime            = 0:01:06.43\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:49,152] [    INFO]\u001b[0m -   train_samples_per_second =       0.03\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:49,152] [    INFO]\u001b[0m -   train_steps_per_second   =       0.03\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:49,155] [    INFO]\u001b[0m - ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:49,155] [    INFO]\u001b[0m -   Num examples = 1\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:49,155] [    INFO]\u001b[0m -   Total prediction steps = 1\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:49,156] [    INFO]\u001b[0m -   Pre device batch size = 16\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:49,156] [    INFO]\u001b[0m -   Total Batch size = 16\u001b[0m\n",
      "\u001b[35m[2023-05-11 12:48:53,163] [   DEBUG]\u001b[0m - in mlflow loss\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:53,849] [    INFO]\u001b[0m - eval_loss: 0.018632939085364342, eval_precision: 0.0, eval_recall: 0.0, eval_f1: 0.0, eval_runtime: 4.6907, eval_samples_per_second: 0.213, eval_steps_per_second: 0.213, epoch: 2.0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:53,849] [    INFO]\u001b[0m - ***** eval metrics *****\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:53,849] [    INFO]\u001b[0m -   epoch                   =        2.0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:53,849] [    INFO]\u001b[0m -   eval_f1                 =        0.0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:53,849] [    INFO]\u001b[0m -   eval_loss               =     0.0186\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:53,850] [    INFO]\u001b[0m -   eval_precision          =        0.0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:53,850] [    INFO]\u001b[0m -   eval_recall             =        0.0\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:53,850] [    INFO]\u001b[0m -   eval_runtime            = 0:00:04.69\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:53,850] [    INFO]\u001b[0m -   eval_samples_per_second =      0.213\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:53,850] [    INFO]\u001b[0m -   eval_steps_per_second   =      0.213\u001b[0m\n",
      "\u001b[33m[2023-05-11 12:48:53,850] [ WARNING]\u001b[0m - Missing export_model_dir path. Using /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/export as default.\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:48:53,853] [    INFO]\u001b[0m - Exporting inference model to /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/model\u001b[0m\n",
      "\u001b[32m[2023-05-11 12:49:03,050] [    INFO]\u001b[0m - Inference model exported.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# cpu test\n",
    "!python3 ../information_extraction/model/finetune.py  \\\n",
    "    --device cpu \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --eval_steps 100 \\\n",
    "    --seed 42 \\\n",
    "    --model_name_or_path uie-base  \\\n",
    "    --train_path ./data/cpu_data/training_data.txt \\\n",
    "    --dev_path ./data/cpu_data/eval_data.txt  \\\n",
    "    --max_seq_len 512  \\\n",
    "    --per_device_eval_batch_size 16\\\n",
    "    --per_device_train_batch_size  16 \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --label_names 'start_positions' 'end_positions' \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_export \\\n",
    "    --overwrite_output_dir \\\n",
    "    --disable_tqdm True \\\n",
    "    --metric_for_best_model eval_f1 \\\n",
    "    --load_best_model_at_end  True \\\n",
    "    --save_total_limit 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d58828fd0c5f7af717daf8982e0a9ccf3c174b5c7bbe63b6216d1f875908829"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
