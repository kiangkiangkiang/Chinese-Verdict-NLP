{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "!python3 ../../../NLLP/experiment/PaddleNLP_UIE/finetune.py  \\\n",
    "    --device cpu \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --eval_steps 100 \\\n",
    "    --seed 42 \\\n",
    "    --model_name_or_path uie-base  \\\n",
    "    --output_dir ../../../NLLP/experiment/PaddleNLP_UIE/checkpoint/model_best \\\n",
    "    --train_path ../../../NLLP/experiment/PaddleNLP_UIE/training_data.txt \\\n",
    "    --dev_path ../../../NLLP/experiment/PaddleNLP_UIE/eval_data.txt  \\\n",
    "    --max_seq_length 512  \\\n",
    "    --per_device_eval_batch_size 16\\\n",
    "    --per_device_train_batch_size  16 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --label_names 'start_positions' 'end_positions' \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_export \\\n",
    "    --do_compress \\\n",
    "    --export_model_dir ../../../NLLP/experiment/PaddleNLP_UIE/checkpoint/model_best \\\n",
    "    --overwrite_output_dir \\\n",
    "    --disable_tqdm True \\\n",
    "    --metric_for_best_model eval_f1 \\\n",
    "    --load_best_model_at_end  True \\\n",
    "    --save_total_limit 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cfh00892302/Desktop/myWorkspace/env/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[33m[2023-05-04 17:10:04,289] [ WARNING]\u001b[0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,289] [    INFO]\u001b[0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,290] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,290] [    INFO]\u001b[0m -      Model Configuration Arguments      \u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,290] [    INFO]\u001b[0m - paddle commit id              :0e92adceae06b6b7463f2dc7790ffb0601730009\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,290] [    INFO]\u001b[0m - export_model_dir              :None\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,290] [    INFO]\u001b[0m - model_name_or_path            :../results/checkpoint\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,290] [    INFO]\u001b[0m - multilingual                  :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,290] [    INFO]\u001b[0m - \u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,290] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,290] [    INFO]\u001b[0m -       Data Configuration Arguments      \u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,290] [    INFO]\u001b[0m - paddle commit id              :0e92adceae06b6b7463f2dc7790ffb0601730009\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,290] [    INFO]\u001b[0m - dev_path                      :../../dev/data/cpu_data/eval_data.txt\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,291] [    INFO]\u001b[0m - max_seq_len                   :512\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,291] [    INFO]\u001b[0m - train_path                    :../../dev/data/cpu_data/training_data.txt\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,291] [    INFO]\u001b[0m - \u001b[0m\n",
      "2023-05-04 17:10:04 |\u001b[32m INFO     \u001b[0m| __main__ | Process rank: -1, device: cpu, world_size: 1, distributed training: False, 16-bits training: False\n",
      "[2023-05-04 17:10:04,291] [    INFO] finetune.py:74 - Process rank: -1, device: cpu, world_size: 1, distributed training: False, 16-bits training: False\n",
      "\u001b[32m[2023-05-04 17:10:04,293] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load '../results/checkpoint'.\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,321] [    INFO]\u001b[0m - loading configuration file ../results/checkpoint/config.json\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:04,322] [    INFO]\u001b[0m - Model config ErnieConfig {\n",
      "  \"architectures\": [\n",
      "    \"UIE\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"enable_recompute\": false,\n",
      "  \"fuse\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"ernie\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"pool_act\": \"tanh\",\n",
      "  \"task_id\": 0,\n",
      "  \"task_type_vocab_size\": 3,\n",
      "  \"type_vocab_size\": 4,\n",
      "  \"use_task_id\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,366] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing UIE.\n",
      "\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,366] [    INFO]\u001b[0m - All the weights of UIE were initialized from the model checkpoint at ../results/checkpoint.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UIE for predictions without further training.\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,641] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,641] [    INFO]\u001b[0m -     Training Configuration Arguments    \u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,641] [    INFO]\u001b[0m - paddle commit id              :0e92adceae06b6b7463f2dc7790ffb0601730009\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,642] [    INFO]\u001b[0m - _no_sync_in_gradient_accumulation:True\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,642] [    INFO]\u001b[0m - adam_beta1                    :0.9\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,642] [    INFO]\u001b[0m - adam_beta2                    :0.999\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,642] [    INFO]\u001b[0m - adam_epsilon                  :1e-08\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,642] [    INFO]\u001b[0m - bf16                          :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,642] [    INFO]\u001b[0m - bf16_full_eval                :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,642] [    INFO]\u001b[0m - current_device                :cpu\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,642] [    INFO]\u001b[0m - dataloader_drop_last          :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,642] [    INFO]\u001b[0m - dataloader_num_workers        :0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,642] [    INFO]\u001b[0m - device                        :cpu\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,642] [    INFO]\u001b[0m - disable_tqdm                  :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,642] [    INFO]\u001b[0m - do_eval                       :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,643] [    INFO]\u001b[0m - do_export                     :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,643] [    INFO]\u001b[0m - do_predict                    :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,643] [    INFO]\u001b[0m - do_train                      :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,643] [    INFO]\u001b[0m - eval_batch_size               :16\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,643] [    INFO]\u001b[0m - eval_steps                    :100\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,643] [    INFO]\u001b[0m - evaluation_strategy           :IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,643] [    INFO]\u001b[0m - flatten_param_grads           :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,643] [    INFO]\u001b[0m - fp16                          :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,643] [    INFO]\u001b[0m - fp16_full_eval                :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,643] [    INFO]\u001b[0m - fp16_opt_level                :O1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,643] [    INFO]\u001b[0m - gradient_accumulation_steps   :1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,643] [    INFO]\u001b[0m - greater_is_better             :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,644] [    INFO]\u001b[0m - ignore_data_skip              :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,644] [    INFO]\u001b[0m - label_names                   :['start_positions', 'end_positions']\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,644] [    INFO]\u001b[0m - lazy_data_processing          :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,644] [    INFO]\u001b[0m - learning_rate                 :1e-05\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,644] [    INFO]\u001b[0m - load_best_model_at_end        :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,644] [    INFO]\u001b[0m - local_process_index           :0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,644] [    INFO]\u001b[0m - local_rank                    :-1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,644] [    INFO]\u001b[0m - log_level                     :-1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,644] [    INFO]\u001b[0m - log_level_replica             :-1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,644] [    INFO]\u001b[0m - log_on_each_node              :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,645] [    INFO]\u001b[0m - logging_dir                   :/Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/runs/May04_17-10-04_CFH00892302de-MacBook-Pro.local\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,645] [    INFO]\u001b[0m - logging_first_step            :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,645] [    INFO]\u001b[0m - logging_steps                 :10\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,645] [    INFO]\u001b[0m - logging_strategy              :IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,645] [    INFO]\u001b[0m - lr_scheduler_type             :SchedulerType.LINEAR\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,645] [    INFO]\u001b[0m - max_grad_norm                 :1.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,645] [    INFO]\u001b[0m - max_steps                     :-1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,645] [    INFO]\u001b[0m - metric_for_best_model         :eval_f1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,645] [    INFO]\u001b[0m - minimum_eval_times            :None\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,646] [    INFO]\u001b[0m - no_cuda                       :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,646] [    INFO]\u001b[0m - num_train_epochs              :1.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,646] [    INFO]\u001b[0m - optim                         :OptimizerNames.ADAMW\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,646] [    INFO]\u001b[0m - output_dir                    :/Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,646] [    INFO]\u001b[0m - overwrite_output_dir          :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,646] [    INFO]\u001b[0m - past_index                    :-1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,646] [    INFO]\u001b[0m - per_device_eval_batch_size    :16\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,646] [    INFO]\u001b[0m - per_device_train_batch_size   :16\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,646] [    INFO]\u001b[0m - prediction_loss_only          :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,647] [    INFO]\u001b[0m - process_index                 :0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,647] [    INFO]\u001b[0m - recompute                     :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,647] [    INFO]\u001b[0m - remove_unused_columns         :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,647] [    INFO]\u001b[0m - report_to                     :['visualdl']\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,647] [    INFO]\u001b[0m - resume_from_checkpoint        :None\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,647] [    INFO]\u001b[0m - run_name                      :/Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,647] [    INFO]\u001b[0m - save_on_each_node             :False\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,647] [    INFO]\u001b[0m - save_steps                    :100\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,647] [    INFO]\u001b[0m - save_strategy                 :IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,647] [    INFO]\u001b[0m - save_total_limit              :1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,647] [    INFO]\u001b[0m - scale_loss                    :32768\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,647] [    INFO]\u001b[0m - seed                          :42\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,648] [    INFO]\u001b[0m - sharding                      :[]\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,648] [    INFO]\u001b[0m - sharding_degree               :-1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,648] [    INFO]\u001b[0m - should_log                    :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,648] [    INFO]\u001b[0m - should_save                   :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,648] [    INFO]\u001b[0m - skip_memory_metrics           :True\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,648] [    INFO]\u001b[0m - train_batch_size              :16\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,648] [    INFO]\u001b[0m - warmup_ratio                  :0.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,648] [    INFO]\u001b[0m - warmup_steps                  :0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,648] [    INFO]\u001b[0m - weight_decay                  :0.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,649] [    INFO]\u001b[0m - world_size                    :1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,649] [    INFO]\u001b[0m - \u001b[0m\n",
      "2023-05-04 17:10:17 |\u001b[36m DEBUG    \u001b[0m| __main__ | chechpoint: None\n",
      "[2023-05-04 17:10:17,649] [   DEBUG] finetune.py:162 - chechpoint: None\n",
      "2023-05-04 17:10:17 |\u001b[36m DEBUG    \u001b[0m| __main__ | last_checkpoint: None\n",
      "[2023-05-04 17:10:17,649] [   DEBUG] finetune.py:163 - last_checkpoint: None\n",
      "\u001b[32m[2023-05-04 17:10:17,651] [    INFO]\u001b[0m - ***** Running training *****\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,651] [    INFO]\u001b[0m -   Num examples = 1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,652] [    INFO]\u001b[0m -   Num Epochs = 1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,652] [    INFO]\u001b[0m -   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,652] [    INFO]\u001b[0m -   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,652] [    INFO]\u001b[0m -   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,652] [    INFO]\u001b[0m -   Total optimization steps = 1.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,652] [    INFO]\u001b[0m -   Total num train samples = 1.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:17,654] [    INFO]\u001b[0m -   Number of trainable parameters = 117946370\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:50,883] [    INFO]\u001b[0m - \n",
      "Training completed. \n",
      "\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:50,886] [    INFO]\u001b[0m - train_runtime: 33.229, train_samples_per_second: 0.03, train_steps_per_second: 0.03, train_loss: 0.009737709537148476, epoch: 1.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:50,886] [    INFO]\u001b[0m - Saving model checkpoint to /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:50,889] [    INFO]\u001b[0m - Configuration saved in /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/config.json\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:52,402] [    INFO]\u001b[0m - tokenizer config file saved in /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:52,402] [    INFO]\u001b[0m - Special tokens file saved in /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:52,404] [    INFO]\u001b[0m - ***** train metrics *****\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:52,405] [    INFO]\u001b[0m -   epoch                    =        1.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:52,405] [    INFO]\u001b[0m -   train_loss               =     0.0097\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:52,405] [    INFO]\u001b[0m -   train_runtime            = 0:00:33.22\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:52,405] [    INFO]\u001b[0m -   train_samples_per_second =       0.03\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:52,406] [    INFO]\u001b[0m -   train_steps_per_second   =       0.03\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:52,408] [    INFO]\u001b[0m - ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:52,409] [    INFO]\u001b[0m -   Num examples = 1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:52,409] [    INFO]\u001b[0m -   Total prediction steps = 1\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:52,409] [    INFO]\u001b[0m -   Pre device batch size = 16\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:52,410] [    INFO]\u001b[0m -   Total Batch size = 16\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:54,976] [    INFO]\u001b[0m - eval_loss: 0.021435033529996872, eval_precision: 0.0, eval_recall: 0.0, eval_f1: 0.0, eval_runtime: 2.5641, eval_samples_per_second: 0.39, eval_steps_per_second: 0.39, epoch: 1.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:54,976] [    INFO]\u001b[0m - ***** eval metrics *****\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:54,976] [    INFO]\u001b[0m -   epoch                   =        1.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:54,976] [    INFO]\u001b[0m -   eval_f1                 =        0.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:54,977] [    INFO]\u001b[0m -   eval_loss               =     0.0214\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:54,977] [    INFO]\u001b[0m -   eval_precision          =        0.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:54,977] [    INFO]\u001b[0m -   eval_recall             =        0.0\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:54,977] [    INFO]\u001b[0m -   eval_runtime            = 0:00:02.56\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:54,977] [    INFO]\u001b[0m -   eval_samples_per_second =       0.39\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:10:54,977] [    INFO]\u001b[0m -   eval_steps_per_second   =       0.39\u001b[0m\n",
      "2023-05-04 17:10:54 |\u001b[33m WARNING  \u001b[0m| __main__ | Missing export_model_dir path. Using /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/export as default.\n",
      "[2023-05-04 17:10:54,977] [ WARNING] finetune.py:196 - Missing export_model_dir path. Using /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/export as default.\n",
      "\u001b[32m[2023-05-04 17:10:54,979] [    INFO]\u001b[0m - Exporting inference model to /Users/cfh00892302/Desktop/myWorkspace/Chinese-Verdict-NLP/information_extraction/results/checkpoint/model\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:11:03,247] [    INFO]\u001b[0m - Inference model exported.\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:11:03,256] [    INFO]\u001b[0m - Model config ErnieConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"enable_recompute\": false,\n",
      "  \"fuse\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"ernie\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"pool_act\": \"tanh\",\n",
      "  \"task_id\": 0,\n",
      "  \"task_type_vocab_size\": 3,\n",
      "  \"type_vocab_size\": 4,\n",
      "  \"use_task_id\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:11:16,064] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing UIE.\n",
      "\u001b[0m\n",
      "\u001b[32m[2023-05-04 17:11:16,064] [    INFO]\u001b[0m - All the weights of UIE were initialized from the model checkpoint at uie-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UIE for predictions without further training.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# toy test\n",
    "!python3 finetune.py  \\\n",
    "    --device cpu \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --eval_steps 100 \\\n",
    "    --seed 42 \\\n",
    "    --model_name_or_path ../results/checkpoint  \\\n",
    "    --train_path ../../dev/data/cpu_data/training_data.txt \\\n",
    "    --dev_path ../../dev/data/cpu_data/eval_data.txt  \\\n",
    "    --max_seq_len 512  \\\n",
    "    --per_device_eval_batch_size 16\\\n",
    "    --per_device_train_batch_size  16 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --label_names 'start_positions' 'end_positions' \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_export \\\n",
    "    --overwrite_output_dir \\\n",
    "    --disable_tqdm True \\\n",
    "    --metric_for_best_model eval_f1 \\\n",
    "    --load_best_model_at_end  True \\\n",
    "    --save_total_limit 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\"薪資費用\", \"醫療費用\", \"精神慰撫金額\"]\n",
    "\n",
    "mytext = \"臺灣橋頭地方法院民事判決108年度訴字第85書記官黃進遠\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/checkpoint/config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpprint\u001b[39;00m \u001b[39mimport\u001b[39;00m pprint\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpaddlenlp\u001b[39;00m \u001b[39mimport\u001b[39;00m Taskflow\n\u001b[0;32m----> 4\u001b[0m my_ie \u001b[39m=\u001b[39m Taskflow(\u001b[39m\"\u001b[39;49m\u001b[39minformation_extraction\u001b[39;49m\u001b[39m\"\u001b[39;49m, schema\u001b[39m=\u001b[39;49mschema, \n\u001b[1;32m      5\u001b[0m                  task_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../results/checkpoint\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m                  precision\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfp32\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m pprint(my_ie(mytext))\n",
      "File \u001b[0;32m~/Desktop/myWorkspace/env/lib/python3.9/site-packages/paddlenlp/taskflow/taskflow.py:696\u001b[0m, in \u001b[0;36mTaskflow.__init__\u001b[0;34m(self, task, model, mode, device_id, from_hf_hub, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs \u001b[39m=\u001b[39m kwargs\n\u001b[1;32m    695\u001b[0m task_class \u001b[39m=\u001b[39m TASKS[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask][tag][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel][\u001b[39m\"\u001b[39m\u001b[39mtask_class\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 696\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask_instance \u001b[39m=\u001b[39m task_class(\n\u001b[1;32m    697\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, task\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtask, priority_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpriority_path, from_hf_hub\u001b[39m=\u001b[39;49mfrom_hf_hub, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs\n\u001b[1;32m    698\u001b[0m )\n\u001b[1;32m    699\u001b[0m task_list \u001b[39m=\u001b[39m TASKS\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m    700\u001b[0m Taskflow\u001b[39m.\u001b[39mtask_list \u001b[39m=\u001b[39m task_list\n",
      "File \u001b[0;32m~/Desktop/myWorkspace/env/lib/python3.9/site-packages/paddlenlp/taskflow/information_extraction.py:415\u001b[0m, in \u001b[0;36mUIETask.__init__\u001b[0;34m(self, task, model, schema, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_task_files()\n\u001b[0;32m--> 415\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_task_path, CONFIG_NAME)) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    416\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_class \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)[\u001b[39m\"\u001b[39m\u001b[39marchitectures\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mpop()\n\u001b[1;32m    418\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_class \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mUIEX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mUIEM\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../results/checkpoint/config.json'"
     ]
    }
   ],
   "source": [
    "# toy inference\n",
    "from pprint import pprint\n",
    "from paddlenlp import Taskflow\n",
    "my_ie = Taskflow(\"information_extraction\", schema=schema, \n",
    "                 task_path='../results/checkpoint',\n",
    "                 precision='fp32')\n",
    "pprint(my_ie(mytext))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu test\n",
    "!python3 finetune.py  \\\n",
    "    --device gpu \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --eval_steps 100 \\\n",
    "    --seed 42 \\\n",
    "    --model_name_or_path uie-base  \\\n",
    "    --train_path ../data/data_for_one_shot_example/training_data.txt \\\n",
    "    --dev_path ../data/data_for_one_shot_example/eval_data.txt  \\\n",
    "    --max_seq_len 512  \\\n",
    "    --per_device_eval_batch_size 16\\\n",
    "    --per_device_train_batch_size  16 \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --label_names 'start_positions' 'end_positions' \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_export \\\n",
    "    --disable_tqdm True \\\n",
    "    --metric_for_best_model eval_f1 \\\n",
    "    --load_best_model_at_end  True \\\n",
    "    --save_total_limit 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d58828fd0c5f7af717daf8982e0a9ccf3c174b5c7bbe63b6216d1f875908829"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
